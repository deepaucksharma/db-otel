# Database Intelligence Collector - With Integrated Verification
# Includes real-time verification and feedback platform
# Version: 2.1.0

extensions:
  # Health monitoring with verification
  health_check:
    endpoint: 0.0.0.0:13133
    path: "/health"
    check_collector_pipeline:
      enabled: true
      interval: 5m
      exporter_failure_threshold: 5
    
  # Performance profiling
  pprof:
    endpoint: 0.0.0.0:1777
    
  # Internal metrics and verification
  zpages:
    endpoint: 0.0.0.0:55679
    
  # File storage for persistence
  file_storage:
    directory: /var/lib/otel/file_storage
    timeout: 10s
    compaction:
      on_start: true
      directory: /var/lib/otel/file_storage
      max_transaction_size: 65536

receivers:
  # PostgreSQL query plan receiver
  sqlquery/postgresql_plans:
    driver: postgres
    dsn: "${env:PG_REPLICA_DSN}"
    collection_interval: "${env:COLLECTION_INTERVAL_SECONDS}s"
    timeout: "${env:QUERY_TIMEOUT_MS}ms"
    storage: file_storage
    
    queries:
      - sql: |
          SET LOCAL statement_timeout = '${env:QUERY_TIMEOUT_MS}ms';
          SET LOCAL lock_timeout = '100ms';
          SET LOCAL idle_in_transaction_session_timeout = '1000ms';
          
          WITH database_queries AS (
            SELECT 
              datname AS database_name,
              pid AS backend_pid,
              usename AS username,
              application_name,
              client_addr,
              backend_start,
              xact_start AS transaction_start,
              query_start,
              state_change AS last_state_change,
              wait_event_type,
              wait_event,
              state AS connection_state,
              backend_xid AS transaction_id,
              backend_xmin AS snapshot_xmin,
              LEFT(query, 
                CASE 
                  WHEN LENGTH(query) > 2000 THEN 2000 
                  ELSE LENGTH(query) 
                END
              ) AS query_text,
              backend_type,
              EXTRACT(EPOCH FROM (now() - query_start)) * 1000 AS duration_ms,
              EXTRACT(EPOCH FROM (now() - xact_start)) * 1000 AS transaction_duration_ms,
              current_timestamp AS collection_timestamp
            FROM pg_stat_activity
            WHERE query NOT LIKE '%pg_stat_activity%'
              AND state = 'active'
              AND backend_type = 'client backend'
              AND EXTRACT(EPOCH FROM (now() - query_start)) > 0.1
          )
          SELECT 
            dq.*,
            pg_size_pretty(pg_database_size(dq.database_name)) AS database_size
          FROM database_queries dq
          ORDER BY duration_ms DESC
          LIMIT 100;
          
  # Self-monitoring metrics
  prometheus:
    config:
      scrape_configs:
        - job_name: 'otel-collector'
          scrape_interval: 30s
          static_configs:
            - targets: ['0.0.0.0:8888']

processors:
  # CRITICAL: Memory protection must be first
  memory_limiter:
    check_interval: 1s
    limit_percentage: 75
    spike_limit_percentage: 20
    ballast_size_mib: 256
    
  # Stage 1: Query normalization and cardinality reduction
  transform/query_normalization:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          # Normalize query text
          - set(attributes["db.query.normalized"], 
              RegexReplace(body["query_text"], "\\b\\d+\\b", "?"))
          
          # Create query fingerprint
          - set(attributes["db.query.fingerprint"], 
              SHA256(attributes["db.query.normalized"]))
          
          # Classify query complexity
          - set(attributes["db.query.complexity"], "simple")
              where body["query_text"] matches "^SELECT.*FROM\\s+\\w+\\s*$"
          - set(attributes["db.query.complexity"], "moderate")
              where body["query_text"] matches "JOIN|GROUP BY|ORDER BY"
          - set(attributes["db.query.complexity"], "complex")
              where body["query_text"] matches "WITH|UNION|INTERSECT|EXCEPT"
              
          # Set sampling priority
          - set(attributes["sampling.priority"], 100)
              where body["duration_ms"] > 1000
          - set(attributes["sampling.priority"], 50)
              where body["duration_ms"] > 500 and body["duration_ms"] <= 1000
          - set(attributes["sampling.priority"], 10)
              where body["duration_ms"] <= 500
              
  # Stage 2: PII sanitization
  transform/pii_sanitization:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          # Remove email addresses
          - set(body["query_text"], 
              RegexReplace(body["query_text"], 
              "[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}", "[EMAIL]"))
          
          # Remove SSNs
          - set(body["query_text"], 
              RegexReplace(body["query_text"], 
              "\\b\\d{3}-\\d{2}-\\d{4}\\b", "[SSN]"))
          
          # Remove credit card numbers
          - set(body["query_text"], 
              RegexReplace(body["query_text"], 
              "\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b", "[CARD]"))
              
  # Stage 3: Entity synthesis for New Relic
  resource/entity_synthesis:
    attributes:
      # Entity identification
      - key: "entity.guid"
        value: "DATABASE|${env:DEPLOYMENT_ENV}|${database_name}"
        action: upsert
        
      - key: "entity.type"
        value: "DATABASE"
        action: insert
        
      - key: "entity.name"
        from_attribute: "database_name"
        action: insert
        
      # Service correlation
      - key: "service.name"
        value: "database-intelligence-${database_name}"
        action: insert
        
      # Infrastructure correlation
      - key: "host.id"
        value: "${env:HOSTNAME}"
        action: insert
        
      # Required New Relic attributes
      - key: "instrumentation.provider"
        value: "opentelemetry"
        action: insert
        
      - key: "telemetry.sdk.name"
        value: "opentelemetry"
        action: insert
        
      - key: "telemetry.sdk.version"
        value: "0.89.0"
        action: insert
        
      - key: "collector.name"
        value: "database-intelligence"
        action: insert
        
  # Stage 4: Circuit breaker implementation
  transform/circuit_breaker:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          # Track errors per database
          - set(cache["cb_errors_" + body["database_name"]], 
              Int(cache["cb_errors_" + body["database_name"]]) + 1)
              where body["error"] == true
          
          # Reset on success
          - set(cache["cb_errors_" + body["database_name"]], 0)
              where body["error"] != true
              
          # Open circuit after threshold
          - set(cache["cb_state_" + body["database_name"]], "open")
              where Int(cache["cb_errors_" + body["database_name"]]) > 5
              
          # Add circuit breaker state
          - set(attributes["cb.state"], cache["cb_state_" + body["database_name"]])
              where cache["cb_state_" + body["database_name"]] != nil
              
          # Drop data when circuit is open
          - set(attributes["drop"], true)
              where cache["cb_state_" + body["database_name"]] == "open"
              
  # Stage 5: VERIFICATION PROCESSOR - Real-time feedback
  verification:
    enable_periodic_verification: true
    verification_interval: 5m
    data_freshness_threshold: 10m
    min_entity_correlation_rate: 0.8
    min_normalization_rate: 0.9
    require_entity_synthesis: true
    export_feedback_as_logs: true
    
    verification_queries:
      - name: "integration_errors"
        query: "SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' SINCE 5 minutes ago"
        interval: 5m
        threshold: 10
        comparison: "lt"
        
      - name: "data_freshness"
        query: "SELECT count(*) FROM Log WHERE collector.name = 'database-intelligence' SINCE 5 minutes ago"
        interval: 5m
        threshold: 1
        comparison: "gt"
        
      - name: "entity_correlation"
        query: "SELECT percentage(count(*), WHERE entity.guid IS NOT NULL) FROM Log WHERE database_name IS NOT NULL SINCE 5 minutes ago"
        interval: 5m
        threshold: 80
        comparison: "gt"
        
  # Stage 6: Adaptive sampling
  probabilistic_sampler/adaptive:
    hash_seed: 22
    attribute_source: record
    from_attribute: "sampling.priority"
    
  # Stage 7: Attribute management
  attributes/cleanup:
    actions:
      # Remove high-cardinality attributes
      - key: backend_pid
        action: delete
      - key: transaction_id
        action: delete
      - key: snapshot_xmin
        action: delete
      
      # Hash sensitive attributes
      - key: client_addr
        action: hash
      - key: username
        action: hash
        
  # Stage 8: Metric generation
  metricgeneration:
    rules:
      - name: db.query.duration
        type: histogram
        unit: ms
        value_attribute: duration_ms
        attributes:
          - database_name
          - db.query.complexity
          - wait_event_type
          
      - name: db.transaction.duration
        type: histogram
        unit: ms
        value_attribute: transaction_duration_ms
        attributes:
          - database_name
          - application_name
          
  # Stage 9: Cardinality control
  groupbyattrs:
    keys:
      - database_name
      - db.query.fingerprint
      - db.query.complexity
      - wait_event_type
      - connection_state
      
  # Stage 10: Optimized batching
  batch/newrelic_optimized:
    timeout: 30s
    send_batch_size: 5000
    send_batch_max_size: 9000

exporters:
  # New Relic optimized exporter
  otlp/newrelic:
    endpoint: "${env:OTLP_ENDPOINT:-https://otlp.nr-data.net:4318}"
    protocol: "http/protobuf"
    compression: zstd
    
    headers:
      api-key: "${env:NEW_RELIC_LICENSE_KEY}"
      "X-Source": "database-intelligence-mvp"
      "X-Version": "2.1.0"
      
    timeout: 25s
    
    sending_queue:
      enabled: true
      num_consumers: 10
      queue_size: 5000
      storage: file_storage
      
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      randomization_factor: 0.5
      multiplier: 1.5
      max_interval: 30s
      max_elapsed_time: 300s
      
  # Debug exporter for troubleshooting
  debug:
    verbosity: basic
    sampling_initial: 10
    sampling_thereafter: 100
    
  # Prometheus for self-monitoring
  prometheusremotewrite:
    endpoint: "${env:PROMETHEUS_ENDPOINT:-http://prometheus:9090/api/v1/write}"
    resource_to_telemetry_conversion:
      enabled: true

service:
  extensions: [health_check, pprof, zpages, file_storage]
  
  telemetry:
    logs:
      level: "${env:LOG_LEVEL:-info}"
      encoding: json
      output_paths: ["stdout", "/var/log/otel-collector.log"]
      
    metrics:
      level: detailed
      address: 0.0.0.0:8888
      
  pipelines:
    # Main database intelligence pipeline with verification
    logs/database:
      receivers: [sqlquery/postgresql_plans]
      processors:
        - memory_limiter
        - transform/query_normalization
        - transform/pii_sanitization
        - resource/entity_synthesis
        - transform/circuit_breaker
        - verification  # Real-time verification and feedback
        - probabilistic_sampler/adaptive
        - attributes/cleanup
        - groupbyattrs
        - batch/newrelic_optimized
      exporters: [otlp/newrelic]
      
    # Metrics pipeline
    metrics/database:
      receivers: [sqlquery/postgresql_plans]
      processors:
        - memory_limiter
        - metricgeneration
        - batch/newrelic_optimized
      exporters: [otlp/newrelic]
      
    # Verification feedback pipeline
    logs/verification:
      receivers: []  # Fed by verification processor
      processors:
        - memory_limiter
        - resource/entity_synthesis
        - batch/newrelic_optimized
      exporters: [otlp/newrelic]
      
    # Self-monitoring pipeline
    metrics/internal:
      receivers: [prometheus]
      processors:
        - memory_limiter
        - resource/entity_synthesis
        - batch/newrelic_optimized
      exporters: [prometheusremotewrite]