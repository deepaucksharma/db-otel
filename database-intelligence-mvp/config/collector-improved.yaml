# New Relic Database Intelligence MVP - FIXED Configuration
# Addresses all critical issues from implementation review

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
  
  # Removed file_storage to enable multi-instance capability
  memory_ballast:
    size_mib: 128
  
  zpages:
    endpoint: 0.0.0.0:55679

receivers:
  # PostgreSQL - FIXED: No custom functions required
  sqlquery/postgresql_plans:
    driver: postgres
    dsn: "${env:PG_REPLICA_DSN}"
    
    # Conservative settings for production
    collection_interval: 300s  # 5 minutes
    timeout: 10s
    
    # Connection safety
    max_open_connections: 2
    max_idle_connections: 1
    connection_max_lifetime: 300s
    connection_max_idle_time: 60s
    
    queries:
      - sql: |
          -- SAFETY FIRST: Mandatory timeouts
          SET LOCAL statement_timeout = '3000ms';
          SET LOCAL lock_timeout = '100ms';
          
          -- Find worst performing query without custom functions
          WITH worst_query AS (
            SELECT 
              queryid,
              query,
              mean_exec_time,
              calls,
              total_exec_time,
              -- Safely truncate long queries
              CASE 
                WHEN length(query) > 1000 THEN 
                  left(query, 1000) || '...[TRUNCATED]'
                ELSE query 
              END as safe_query,
              (mean_exec_time * calls) as impact_score
            FROM pg_stat_statements
            WHERE 
              mean_exec_time > 50        -- 50ms+ queries
              AND calls > 5              -- Some frequency
              AND query NOT LIKE '%pg_%' -- Skip system queries
              AND query NOT LIKE '%EXPLAIN%' -- Prevent recursion
              AND query NOT LIKE '%SET LOCAL%' -- Skip our queries
              AND query NOT LIKE '%information_schema%'
              AND length(query) > 20
              AND length(query) < 5000   -- Reasonable size
            ORDER BY impact_score DESC
            LIMIT 1
          )
          SELECT 
            queryid::text as query_id,
            safe_query as query_text,
            round(mean_exec_time::numeric, 2) as avg_duration_ms,
            calls as execution_count,
            round(total_exec_time::numeric, 2) as total_duration_ms,
            round(impact_score::numeric, 2) as impact_score,
            -- Metadata-only approach (no EXPLAIN for safety)
            json_build_object(
              'query_id', queryid::text,
              'system', 'postgresql',
              'avg_duration_ms', round(mean_exec_time::numeric, 2),
              'execution_count', calls,
              'impact_score', round(impact_score::numeric, 2),
              'collection_time', now()::text,
              'database', current_database(),
              'plan_available', false,
              'approach', 'metadata_only_for_safety'
            ) as plan_metadata
          FROM worst_query;

  # MySQL - Performance Schema approach
  sqlquery/mysql_plans:
    driver: mysql
    dsn: "${env:MYSQL_READONLY_DSN}"
    collection_interval: 300s
    timeout: 10s
    
    max_open_connections: 2
    max_idle_connections: 1
    connection_max_lifetime: 300s
    
    queries:
      - sql: |
          SELECT 
            DIGEST as query_id,
            DIGEST_TEXT as query_text,
            ROUND(AVG_TIMER_WAIT/1000000, 2) as avg_duration_ms,
            COUNT_STAR as execution_count,
            ROUND((AVG_TIMER_WAIT * COUNT_STAR)/1000000, 2) as total_duration_ms,
            JSON_OBJECT(
              'query_id', DIGEST,
              'system', 'mysql',
              'schema', SCHEMA_NAME,
              'avg_duration_ms', ROUND(AVG_TIMER_WAIT/1000000, 2),
              'execution_count', COUNT_STAR,
              'avg_rows_examined', ROUND(SUM_ROWS_EXAMINED/COUNT_STAR, 0),
              'avg_rows_sent', ROUND(SUM_ROWS_SENT/COUNT_STAR, 0),
              'collection_time', NOW(),
              'plan_available', false,
              'approach', 'performance_schema_metadata'
            ) as plan_metadata
          FROM performance_schema.events_statements_summary_by_digest
          WHERE 
            SCHEMA_NAME = DATABASE()
            AND SCHEMA_NAME IS NOT NULL
            AND AVG_TIMER_WAIT > 50000000  -- 50ms+
            AND COUNT_STAR > 5
            AND DIGEST_TEXT NOT LIKE 'SELECT @@%'
            AND DIGEST_TEXT NOT LIKE 'SHOW %'
            AND DIGEST_TEXT NOT LIKE 'EXPLAIN %'
            AND LENGTH(DIGEST_TEXT) > 20
          ORDER BY (AVG_TIMER_WAIT * COUNT_STAR) DESC
          LIMIT 1;

  # File log receiver for auto_explain (zero impact)
  filelog/postgresql_auto_explain:
    include: 
      - /var/log/postgresql/postgresql-*.log
      - /var/log/postgres/*.log
    exclude:
      - /var/log/postgresql/*.gz
      - /var/log/postgres/*.gz
    
    start_at: end
    include_file_name: false
    include_file_path: true
    
    multiline:
      line_start_pattern: '^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}'
      max_log_lines: 100
      timeout: 5s
    
    operators:
      - type: regex_parser
        regex: '^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}).*duration: (?P<duration>\d+\.\d+) ms.*plan:\s*(?P<plan>\{.*\})'
        on_error: send
        
      - type: json_parser
        parse_from: attributes.plan
        parse_to: body
        on_error: send_quiet
        
      - type: add
        fields:
          attributes.db_system: postgresql
          attributes.db_source: auto_explain
          attributes.collection_method: zero_impact

processors:
  # ALWAYS FIRST: Memory protection
  memory_limiter:
    check_interval: 2s
    limit_mib: 1024
    spike_limit_mib: 256

  # Circuit breaker for safety
  transform/circuit_breaker:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          # Track error rate
          - set(attributes["circuit_breaker_errors"], 0) where attributes["circuit_breaker_errors"] == nil
          # Will implement proper circuit breaker logic

  # Enhanced PII sanitization
  transform/sanitize_pii:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          # Email addresses
          - replace_all_patterns(
              body,
              "\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b",
              "[EMAIL_REDACTED]"
            )
          # Phone numbers
          - replace_all_patterns(
              body,
              "\\b(?:\\+1[\\s-]?)?\\(?\\d{3}\\)?[\\s-]?\\d{3}[\\s-]?\\d{4}\\b",
              "[PHONE_REDACTED]"
            )
          # SSN
          - replace_all_patterns(
              body,
              "\\b\\d{3}-\\d{2}-\\d{4}\\b",
              "[SSN_REDACTED]"
            )
          # Credit cards
          - replace_all_patterns(
              body,
              "\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b",
              "[CARD_REDACTED]"
            )
          # SQL literals in query text
          - replace_all_patterns(
              attributes["query_text"],
              "'[^']*'",
              "'[REDACTED]'"
            )
          # Large numbers (potential IDs)
          - replace_all_patterns(
              attributes["query_text"],
              "\\b\\d{6,}\\b",
              "[ID]"
            )

  # Attribute enhancement
  transform/extract_attributes:
    log_statements:
      - context: log
        statements:
          # Database system detection
          - set(attributes["db_system"], "postgresql") where attributes["query_id"] != nil and IsString(body) and body contains "postgresql"
          - set(attributes["db_system"], "mysql") where attributes["query_id"] != nil and IsString(body) and body contains "mysql"
          
          # Query type classification
          - set(attributes["db_query_type"], "SELECT") where IsString(attributes["query_text"]) and attributes["query_text"] matches "(?i)^\\s*SELECT"
          - set(attributes["db_query_type"], "INSERT") where IsString(attributes["query_text"]) and attributes["query_text"] matches "(?i)^\\s*INSERT"
          - set(attributes["db_query_type"], "UPDATE") where IsString(attributes["query_text"]) and attributes["query_text"] matches "(?i)^\\s*UPDATE"
          - set(attributes["db_query_type"], "DELETE") where IsString(attributes["query_text"]) and attributes["query_text"] matches "(?i)^\\s*DELETE"
          
          # Performance classification
          - set(attributes["db_query_performance"], "critical") where IsString(attributes["avg_duration_ms"]) and Double(attributes["avg_duration_ms"]) > 1000
          - set(attributes["db_query_performance"], "slow") where IsString(attributes["avg_duration_ms"]) and Double(attributes["avg_duration_ms"]) > 100 and Double(attributes["avg_duration_ms"]) <= 1000
          - set(attributes["db_query_performance"], "normal") where IsString(attributes["avg_duration_ms"]) and Double(attributes["avg_duration_ms"]) <= 100
          
          # Generate fingerprint for deduplication
          - set(attributes["db_query_fingerprint"], SHA256(attributes["query_text"])) where attributes["query_text"] != nil
          
          # Add collection metadata
          - set(attributes["collection_timestamp"], Now())
          - set(attributes["collector_version"], "mvp-1.0-fixed")

  # Intelligent sampling (stateless for multi-instance)
  probabilistic_sampler:
    hash_seed: 22
    sampling_percentage: 25  # Sample 25% initially

  # Batching
  batch:
    timeout: 30s
    send_batch_size: 50
    send_batch_max_size: 100

exporters:
  # New Relic OTLP
  otlp/newrelic:
    endpoint: "${env:OTLP_ENDPOINT:-https://otlp.nr-data.net:4317}"
    headers:
      api-key: "${env:NEW_RELIC_LICENSE_KEY}"
    
    compression: gzip
    timeout: 30s
    
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 120s
      
    sending_queue:
      enabled: true
      num_consumers: 2
      queue_size: 256

  # Debug logging
  logging:
    loglevel: info
    sampling_initial: 5
    sampling_thereafter: 100

service:
  extensions: [health_check, memory_ballast, zpages]
  
  pipelines:
    logs/database_intelligence:
      receivers: 
        - sqlquery/postgresql_plans
        - sqlquery/mysql_plans
        - filelog/postgresql_auto_explain
      processors:
        - memory_limiter
        - transform/circuit_breaker
        - transform/sanitize_pii
        - transform/extract_attributes
        - probabilistic_sampler
        - batch
      exporters:
        - otlp/newrelic
        - logging

  telemetry:
    logs:
      level: info
      development: false
      encoding: json
      output_paths: [stdout]
      error_output_paths: [stderr]
      
    metrics:
      level: detailed
      address: 0.0.0.0:8888
      
    traces:
      level: none