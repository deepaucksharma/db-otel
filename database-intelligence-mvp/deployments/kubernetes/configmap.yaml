apiVersion: v1
kind: ConfigMap
metadata:
  name: nr-db-intelligence-config
  namespace: monitoring
  labels:
    app: nr-db-intelligence
    component: config
    version: mvp-1.0
data:
  config.yaml: |
    # Database Intelligence MVP - OpenTelemetry Collector Configuration
    # Production deployment configuration
    
    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
      
      file_storage:
        directory: /var/lib/otel/storage
        timeout: 10s
        compaction:
          directory: /var/lib/otel/storage/compaction
          on_start: true
          on_rebound: true
          rebound_needed_threshold_mib: 100
          rebound_trigger_threshold_mib: 10
      
      zpages:
        endpoint: 0.0.0.0:55679

    receivers:
      # PostgreSQL Query Plan Receiver
      sqlquery/postgresql_plans_safe:
        driver: postgres
        dsn: "${env:PG_REPLICA_DSN}"
        
        # Production Safety Settings
        collection_interval: 60s
        timeout: 5s
        max_idle_time: 60s
        max_lifetime: 300s
        max_open_connections: 2
        max_idle_connections: 1
        
        queries:
        - sql: |
            -- MANDATORY SAFETY TIMEOUTS
            SET LOCAL statement_timeout = '2000';
            SET LOCAL lock_timeout = '100';
            
            -- Production Query: Single worst performer
            WITH worst_query AS (
              SELECT 
                queryid,
                query,
                mean_exec_time,
                calls,
                total_exec_time,
                (mean_exec_time * calls) as impact_score
              FROM pg_stat_statements
              WHERE 
                mean_exec_time > 100
                AND calls > 10
                AND query NOT LIKE '%pg_%'
                AND query NOT LIKE '%EXPLAIN%'
                AND query NOT LIKE '%information_schema%'
                AND length(query) < 4000
              ORDER BY impact_score DESC
              LIMIT 1
            )
            SELECT
              w.queryid::text as query_id,
              w.query as query_text,
              w.mean_exec_time as avg_duration_ms,
              w.calls as execution_count,
              w.total_exec_time as total_duration_ms,
              w.impact_score::bigint as impact_score,
              -- Metadata for processing
              now() as collection_timestamp,
              current_database() as database_name,
              version() as pg_version,
              '{}' as plan_json  -- Placeholder for now
            FROM worst_query w;
      
      # Zero-Impact File Log Collection
      filelog/pg_auto_explain:
        include: 
        - /var/log/postgresql/postgresql-*.log
        - /var/log/postgresql/*.log
        exclude:
        - /var/log/postgresql/*.gz
        - /var/log/postgresql/*.bz2
        
        start_at: end
        max_log_size: 10MiB
        max_concurrent_files: 10
        
        multiline:
          line_start_pattern: '^\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}'
          max_log_size: 1MiB
          timeout: 5s
        
        operators:
        - type: regex_parser
          id: extract_plan
          regex: '^(?P<timestamp>\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}.\d+\s+\w+).*duration:\s+(?P<duration>\d+\.\d+)\s+ms.*plan:\s*(?P<plan>\{.*\}).*$'
          on_error: send
          
        - type: json_parser
          id: parse_plan
          parse_from: attributes.plan
          parse_to: body.plan
          on_error: send_quiet
          
        - type: add
          id: add_metadata
          fields:
            attributes.db.system: postgresql
            attributes.db.source: auto_explain
            attributes.collection.method: zero_impact

    processors:
      # Memory Protection - ALWAYS FIRST
      memory_limiter:
        check_interval: 1s
        limit_mib: 512
        spike_limit_mib: 128
        ballast_size_mib: 64

      # PII Sanitization - ALWAYS SECOND
      transform/sanitize_pii:
        error_mode: ignore
        log_statements:
        - context: log
          statements:
          # Email addresses
          - replace_all_patterns(
              attributes,
              "value",
              "\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b",
              "[EMAIL]"
            )
          
          # Social Security Numbers
          - replace_pattern(body, "\\b\\d{3}-\\d{2}-\\d{4}\\b", "[SSN]")
          - replace_pattern(body, "\\b\\d{9}\\b", "[SSN]")
          
          # Credit Card Numbers
          - replace_pattern(
              body,
              "\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b",
              "[CARD]"
            )
          
          # Phone Numbers
          - replace_pattern(
              body,
              "\\b(?:\\+1[\\s-]?)?\\(?\\d{3}\\)?[\\s-]?\\d{3}[\\s-]?\\d{4}\\b",
              "[PHONE]"
            )
          
          # SQL Literals
          - replace_pattern(
              attributes["query_text"],
              "'[^']*'",
              "'[LITERAL]'"
            )

      # Plan Attribute Extraction (Custom Processor)
      plan_attribute_extractor:
        timeout_ms: 100
        error_mode: ignore
        
        postgresql_rules:
          detection_jsonpath: "$[0].Plan"
          extractions:
            db.query.plan.cost: "$[0].Plan['Total Cost']"
            db.query.plan.rows: "$[0].Plan['Plan Rows']"
            db.query.plan.width: "$[0].Plan['Plan Width']"
            db.query.plan.operation: "$[0].Plan['Node Type']"
            db.query.plan.startup_cost: "$[0].Plan['Startup Cost']"
          
          derived:
            db.query.plan.has_seq_scan: "has_substr_in_plan(plan_json, 'Seq Scan')"
            db.query.plan.has_nested_loop: "has_substr_in_plan(plan_json, 'Nested Loop')"
            db.query.plan.depth: "json_depth(plan_json)"
        
        hash_config:
          include:
          - query_text
          - db.query.plan.operation
          - database_name
          output: db.query.plan.hash
          algorithm: sha256

      # Adaptive Sampling (Custom Processor)
      adaptive_sampler:
        state_storage:
          type: file_storage
          file_storage:
            directory: /var/lib/otel/storage/sampling
            sync_interval: 10s
            compaction_interval: 300s
            max_size_mb: 100
        
        deduplication:
          enabled: true
          cache_size: 10000
          window_seconds: 300
          hash_attribute: db.query.plan.hash
        
        rules:
        # Critical queries - always sample
        - name: critical_queries
          priority: 100
          sample_rate: 1.0
          conditions:
          - attribute: avg_duration_ms
            operator: gt
            value: 1000
        
        # Missing indexes - always sample
        - name: missing_indexes
          priority: 90
          sample_rate: 1.0
          conditions:
          - attribute: db.query.plan.has_seq_scan
            operator: eq
            value: true
          - attribute: db.query.plan.rows
            operator: gt
            value: 10000
        
        # High frequency - reduce sampling
        - name: high_frequency
          priority: 50
          sample_rate: 0.01
          max_per_minute: 10
          conditions:
          - attribute: execution_count
            operator: gt
            value: 1000
        
        # Default rule
        - name: default
          priority: 0
          sample_rate: 0.1
        
        default_sample_rate: 0.1
        max_records_per_second: 1000

      # Metadata Enrichment
      transform/enrich_metadata:
        error_mode: ignore
        log_statements:
        - context: log
          statements:
          - set(attributes["db.intelligence.version"], "mvp-1.0")
          - set(attributes["db.intelligence.collector"], "otel-contrib")
          - set(attributes["collection.timestamp"], Now())
          - set(attributes["kubernetes.pod.name"], "${env:POD_NAME}")
          - set(attributes["kubernetes.namespace"], "${env:POD_NAMESPACE}")

      # Batching for efficiency
      batch:
        timeout: 10s
        send_batch_size: 100
        send_batch_max_size: 500

    exporters:
      # New Relic OTLP Export
      otlp:
        endpoint: "${env:OTLP_ENDPOINT}"
        
        headers:
          api-key: "${env:NEW_RELIC_LICENSE_KEY}"
        
        compression: gzip
        
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
          randomization_factor: 0.5
          multiplier: 1.5
        
        sending_queue:
          enabled: true
          num_consumers: 4
          queue_size: 100
        
        timeout: 30s
        
        tls:
          insecure: false

      # Debug exporter for troubleshooting
      debug:
        verbosity: basic
        sampling_initial: 1
        sampling_thereafter: 100

    service:
      extensions:
      - health_check
      - file_storage
      - zpages
      
      pipelines:
        logs/database_plans:
          receivers:
          - sqlquery/postgresql_plans_safe
          - filelog/pg_auto_explain
          processors:
          - memory_limiter           # Safety first
          - transform/sanitize_pii   # Security second
          - plan_attribute_extractor # Parse plans
          - adaptive_sampler         # Smart sampling
          - transform/enrich_metadata # Add context
          - batch                    # Efficiency last
          exporters:
          - otlp
          # - debug  # Enable for troubleshooting
      
      # Self-monitoring
      telemetry:
        logs:
          level: info
          development: false
          encoding: json
          output_paths: ["/var/log/otel/collector.log"]
          error_output_paths: ["stderr"]
          initial_fields:
            service: database-intelligence-collector
            version: mvp-1.0
            pod_name: "${env:POD_NAME}"
            namespace: "${env:POD_NAMESPACE}"
        
        metrics:
          level: detailed
          address: 0.0.0.0:8888
        
        resource:
          service.name: database-intelligence-collector
          service.version: mvp-1.0
          deployment.environment: "${env:DEPLOYMENT_ENV}"
          k8s.pod.name: "${env:POD_NAME}"
          k8s.namespace.name: "${env:POD_NAMESPACE}"