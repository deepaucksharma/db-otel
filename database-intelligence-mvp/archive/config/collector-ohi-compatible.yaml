# OpenTelemetry Collector Configuration - OHI Compatible
# Implements all PostgreSQL OHI metric samples based on strategy documents

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
    path: "/health"
    
  pprof:
    endpoint: 0.0.0.0:1777
    
  zpages:
    endpoint: 0.0.0.0:55679
    
  file_storage:
    directory: /var/lib/otel/file_storage
    timeout: 10s

receivers:
  # PostgreSQL receiver for basic metrics (PostgreSQLSample equivalent)
  postgresql:
    endpoint: ${env:PG_HOST:-host.docker.internal}:${env:PG_PORT:-5432}
    username: ${env:PG_USER:-newrelic_monitor}
    password: ${env:PG_PASSWORD:-monitor123}
    databases:
      - ${env:PG_DATABASE:-testdb}
    collection_interval: 60s
    tls:
      insecure: true
      insecure_skip_verify: true
      
  # SQL Query receiver for PostgreSQL slow queries (PostgresSlowQueries)
  sqlquery/slow_queries:
    driver: postgres
    dsn: "${env:PG_REPLICA_DSN}"
    collection_interval: 300s  # 5 minutes as per OHI
    timeout: 10s
    storage: file_storage
    queries:
      - sql: |
          -- OHI-compatible slow query collection for PostgreSQL 13+
          SET LOCAL statement_timeout = '3000ms';
          SET LOCAL lock_timeout = '100ms';
          
          SELECT 
            'newrelic' as newrelic,
            pss.queryid::text AS query_id,
            LEFT(pss.query, 4095) AS query_text,
            pd.datname AS database_name,
            current_schema() AS schema_name,
            pss.calls AS execution_count,
            ROUND((pss.total_exec_time / NULLIF(pss.calls, 0))::numeric, 3) AS avg_elapsed_time_ms,
            (pss.shared_blks_read / NULLIF(pss.calls, 0))::float AS avg_disk_reads,
            (pss.shared_blks_written / NULLIF(pss.calls, 0))::float AS avg_disk_writes,
            CASE
              WHEN pss.query ILIKE 'SELECT%%' THEN 'SELECT'
              WHEN pss.query ILIKE 'INSERT%%' THEN 'INSERT'
              WHEN pss.query ILIKE 'UPDATE%%' THEN 'UPDATE'
              WHEN pss.query ILIKE 'DELETE%%' THEN 'DELETE'
              ELSE 'OTHER'
            END AS statement_type,
            to_char(NOW() AT TIME ZONE 'UTC', 'YYYY-MM-DD"T"HH24:MI:SS"Z"') AS collection_timestamp,
            NULL as individual_query
          FROM pg_stat_statements pss
          JOIN pg_database pd ON pss.dbid = pd.oid
          WHERE pd.datname = current_database()
            AND pss.query NOT ILIKE 'EXPLAIN (FORMAT JSON)%%'
            AND pss.query NOT ILIKE 'SELECT $1 as newrelic%%'
            AND pss.query NOT ILIKE 'WITH wait_history AS%%'
            AND pss.calls > ${env:QUERY_MONITORING_COUNT_THRESHOLD:-20}
            AND (pss.total_exec_time / NULLIF(pss.calls, 0)) > ${env:QUERY_MONITORING_RESPONSE_TIME_THRESHOLD:-500}
          ORDER BY avg_elapsed_time_ms DESC
          LIMIT ${env:QUERY_MONITORING_COUNT_THRESHOLD:-20};
        logs:
          - body_column: query_text
            attributes:
              - attribute_column: newrelic
              - attribute_column: query_id
              - attribute_column: database_name
              - attribute_column: schema_name
              - attribute_column: execution_count
              - attribute_column: avg_elapsed_time_ms
              - attribute_column: avg_disk_reads
              - attribute_column: avg_disk_writes
              - attribute_column: statement_type
              - attribute_column: collection_timestamp
              - attribute_column: individual_query

  # SQL Query receiver for wait events (PostgresWaitEvents)
  sqlquery/wait_events:
    driver: postgres
    dsn: "${env:PG_REPLICA_DSN}"
    collection_interval: 300s
    timeout: 10s
    storage: file_storage
    queries:
      - sql: |
          -- Wait events query (requires pg_wait_sampling extension)
          SELECT 
            queryid::text as query_id,
            event_type,
            event,
            count(*) as wait_count,
            sum(duration) as total_wait_time_ms
          FROM pg_wait_sampling_history
          WHERE queryid IS NOT NULL
          GROUP BY queryid, event_type, event
          ORDER BY total_wait_time_ms DESC
          LIMIT 100;
        logs:
          - body_column: event
            attributes:
              - attribute_column: query_id
              - attribute_column: event_type
              - attribute_column: wait_count
              - attribute_column: total_wait_time_ms

  # SQL Query receiver for blocking sessions (PostgresBlockingSessions)
  sqlquery/blocking_sessions:
    driver: postgres
    dsn: "${env:PG_REPLICA_DSN}"
    collection_interval: 60s
    timeout: 5s
    queries:
      - sql: |
          -- Blocking sessions query
          WITH blocking_activity AS (
            SELECT 
              blocked_locks.pid AS blocked_pid,
              blocked_activity.usename AS blocked_user,
              blocking_locks.pid AS blocking_pid,
              blocking_activity.usename AS blocking_user,
              blocked_activity.query AS blocked_query,
              blocking_activity.query AS blocking_query,
              blocked_activity.application_name AS blocked_application,
              blocking_activity.application_name AS blocking_application,
              now() - blocked_activity.query_start AS blocked_duration,
              now() - blocking_activity.query_start AS blocking_duration
            FROM pg_catalog.pg_locks blocked_locks
            JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid
            JOIN pg_catalog.pg_locks blocking_locks 
              ON blocking_locks.locktype = blocked_locks.locktype
              AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database
              AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation
              AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page
              AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple
              AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid
              AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid
              AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid
              AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid
              AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid
              AND blocking_locks.pid != blocked_locks.pid
            JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid
            WHERE NOT blocked_locks.granted
          )
          SELECT * FROM blocking_activity;
        logs:
          - body_column: blocked_query
            attributes:
              - attribute_column: blocked_pid
              - attribute_column: blocked_user
              - attribute_column: blocking_pid
              - attribute_column: blocking_user
              - attribute_column: blocking_query
              - attribute_column: blocked_application
              - attribute_column: blocking_application
              - attribute_column: blocked_duration
              - attribute_column: blocking_duration

  # SQL Query receiver for execution plans (PostgresExecutionPlans)
  sqlquery/execution_plans:
    driver: postgres
    dsn: "${env:PG_REPLICA_DSN}"
    collection_interval: 600s  # 10 minutes
    timeout: 30s
    storage: file_storage
    queries:
      - sql: |
          -- Get execution plans for top queries
          WITH top_queries AS (
            SELECT 
              queryid,
              query,
              mean_exec_time
            FROM pg_stat_statements
            WHERE query NOT LIKE '%pg_stat_statements%'
            ORDER BY mean_exec_time DESC
            LIMIT 10
          )
          SELECT 
            queryid::text as query_id,
            query as query_text,
            mean_exec_time,
            (SELECT json_agg(plan) FROM (
              SELECT * FROM json_array_elements(
                (SELECT plan FROM pg_stat_statements_info WHERE queryid = tq.queryid)
              ) AS plan
            ) p) as execution_plan
          FROM top_queries tq;
        logs:
          - body_column: execution_plan
            attributes:
              - attribute_column: query_id
              - attribute_column: query_text
              - attribute_column: mean_exec_time

  # Prometheus receiver for self-monitoring
  prometheus:
    config:
      scrape_configs:
        - job_name: 'otel-collector'
          scrape_interval: 30s
          static_configs:
            - targets: ['0.0.0.0:8888']

processors:
  # Memory protection
  memory_limiter:
    check_interval: 1s
    limit_percentage: 75
    spike_limit_percentage: 20
    
  # Add standard OHI attributes
  attributes/ohi_compatibility:
    actions:
      - key: eventType
        value: PostgresSlowQueries
        action: insert
        # Only add to slow query logs
        from_context: log
      - key: provider
        value: "ohi"
        action: insert
      - key: nr.provider
        value: "newrelic"
        action: insert
      - key: collector.name
        value: database-intelligence
        action: insert
      - key: instrumentation.provider
        value: opentelemetry
        action: insert
      - key: instrumentation.version
        value: "2.0.0"
        action: insert
        
  # Transform to match OHI event structure
  transform/ohi_events:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          # Add event type based on receiver
          - set(attributes["eventType"], "PostgresSlowQueries") where attributes["query_id"] != nil and attributes["avg_elapsed_time_ms"] != nil
          - set(attributes["eventType"], "PostgresWaitEvents") where attributes["event_type"] != nil and attributes["wait_count"] != nil
          - set(attributes["eventType"], "PostgresBlockingSessions") where attributes["blocked_pid"] != nil
          - set(attributes["eventType"], "PostgresExecutionPlans") where attributes["execution_plan"] != nil
          
          # Add entity synthesis attributes
          - set(attributes["entity.name"], Concat([attributes["database_name"], ":", attributes["schema_name"]], ""))
          - set(attributes["entity.type"], "POSTGRESQL_DATABASE")
          - set(attributes["entity.guid"], Concat(["POSTGRESQL_DATABASE|", attributes["database_name"]], ""))
          
  # Resource processor for entity attributes
  resource:
    attributes:
      - key: service.name
        value: "postgresql"
        action: insert
      - key: service.namespace
        value: ${env:DEPLOYMENT_ENV:-production}
        action: insert
      - key: host.name
        value: ${env:PG_HOST:-localhost}
        action: insert
      - key: db.system
        value: "postgresql"
        action: insert
        
  # Batch for optimal sending
  batch:
    timeout: 30s
    send_batch_size: 1000
    send_batch_max_size: 2000

  # Group by event type for proper New Relic ingestion
  groupbyattrs:
    keys:
      - eventType
      - database_name
      - schema_name

exporters:
  # Debug exporter
  debug:
    verbosity: detailed
    sampling_initial: 5
    sampling_thereafter: 100
    
  # New Relic OTLP exporter
  otlp/newrelic:
    endpoint: "${env:OTLP_ENDPOINT:-https://otlp.nr-data.net:4318}"
    headers:
      api-key: "${env:NEW_RELIC_LICENSE_KEY}"
    sending_queue:
      enabled: true
      num_consumers: 10
      queue_size: 5000
      storage: file_storage
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
      
  # Convert logs to New Relic events format
  newrelic:
    apikey: "${env:NEW_RELIC_LICENSE_KEY}"
    timeout: 30s
    # Map to appropriate event types
    logs:
      common_attributes:
        - key: hostname
          value: ${env:PG_HOST:-localhost}
        - key: environment
          value: ${env:DEPLOYMENT_ENV:-production}

service:
  extensions: [health_check, pprof, zpages, file_storage]
  
  telemetry:
    logs:
      level: "${env:LOG_LEVEL:-info}"
      encoding: json
      
    metrics:
      level: detailed
      address: 0.0.0.0:8888
      
  pipelines:
    # PostgreSQL infrastructure metrics
    metrics/postgresql:
      receivers: [postgresql]
      processors:
        - memory_limiter
        - resource
        - batch
      exporters: [otlp/newrelic, debug]
      
    # Slow queries pipeline
    logs/slow_queries:
      receivers: [sqlquery/slow_queries]
      processors:
        - memory_limiter
        - attributes/ohi_compatibility
        - transform/ohi_events
        - resource
        - groupbyattrs
        - batch
      exporters: [newrelic, debug]
      
    # Wait events pipeline
    logs/wait_events:
      receivers: [sqlquery/wait_events]
      processors:
        - memory_limiter
        - attributes/ohi_compatibility
        - transform/ohi_events
        - resource
        - batch
      exporters: [newrelic, debug]
      
    # Blocking sessions pipeline
    logs/blocking_sessions:
      receivers: [sqlquery/blocking_sessions]
      processors:
        - memory_limiter
        - attributes/ohi_compatibility
        - transform/ohi_events
        - resource
        - batch
      exporters: [newrelic, debug]
      
    # Self-monitoring
    metrics/internal:
      receivers: [prometheus]
      processors:
        - memory_limiter
        - batch
      exporters: [otlp/newrelic]