# Database Intelligence Collector - New Relic Optimized Configuration
# Incorporates all best practices from OpenTelemetry-New Relic integration guide
# Version: 2.0.0

extensions:
  # Health monitoring
  health_check:
    endpoint: 0.0.0.0:13133
    path: "/health"
    
  # Performance profiling
  pprof:
    endpoint: 0.0.0.0:1777
    
  # Internal metrics
  zpages:
    endpoint: 0.0.0.0:55679
    
  # File storage for persistence
  file_storage:
    directory: /var/lib/otel/file_storage
    timeout: 10s
    compaction:
      on_start: true
      directory: /var/lib/otel/file_storage
      max_transaction_size: 65536

receivers:
  # PostgreSQL query plan receiver
  sqlquery/postgresql_plans:
    driver: postgres
    dsn: "${env:PG_REPLICA_DSN}"
    collection_interval: "${env:COLLECTION_INTERVAL_SECONDS}s"
    timeout: "${env:QUERY_TIMEOUT_MS}ms"
    storage: file_storage
    
    queries:
      - sql: |
          SET LOCAL statement_timeout = '${env:QUERY_TIMEOUT_MS}ms';
          SET LOCAL lock_timeout = '100ms';
          SET LOCAL idle_in_transaction_session_timeout = '1000ms';
          
          WITH database_queries AS (
            SELECT 
              datname AS database_name,
              pid AS backend_pid,
              usename AS username,
              application_name,
              client_addr,
              backend_start,
              xact_start AS transaction_start,
              query_start,
              state_change AS last_state_change,
              wait_event_type,
              wait_event,
              state AS connection_state,
              backend_xid AS transaction_id,
              backend_xmin AS snapshot_xmin,
              LEFT(query, 
                CASE 
                  WHEN LENGTH(query) > 2000 THEN 2000 
                  ELSE LENGTH(query) 
                END
              ) AS query_text,
              backend_type,
              EXTRACT(EPOCH FROM (now() - query_start)) * 1000 AS duration_ms,
              EXTRACT(EPOCH FROM (now() - xact_start)) * 1000 AS transaction_duration_ms,
              current_timestamp AS collection_timestamp
            FROM pg_stat_activity
            WHERE query NOT LIKE '%pg_stat_activity%'
              AND state = 'active'
              AND backend_type = 'client backend'
              AND EXTRACT(EPOCH FROM (now() - query_start)) > 0.1
          )
          SELECT 
            dq.*,
            pg_size_pretty(pg_database_size(dq.database_name)) AS database_size
          FROM database_queries dq
          ORDER BY duration_ms DESC
          LIMIT 100;
          
  # Self-monitoring metrics
  prometheus:
    config:
      scrape_configs:
        - job_name: 'otel-collector'
          scrape_interval: 30s
          static_configs:
            - targets: ['0.0.0.0:8888']

processors:
  # CRITICAL: Memory protection must be first
  memory_limiter:
    check_interval: 1s
    limit_percentage: 75
    spike_limit_percentage: 20
    ballast_size_mib: 256  # Prevent GC thrashing
    
  # Stage 1: Query normalization and cardinality reduction
  transform/query_normalization:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          # Normalize numeric values
          - set(attributes["db.query.normalized"], 
              RegexReplace(body["query_text"], "\\b\\d+\\b", "?"))
              where body["query_text"] != nil
          
          # Create query fingerprint
          - set(attributes["db.query.fingerprint"], 
              SHA256(attributes["db.query.normalized"]))
              where attributes["db.query.normalized"] != nil
          
          # Categorize query complexity
          - set(attributes["db.query.complexity"], "simple")
              where Len(Split(body["query_text"], "JOIN")) <= 2
          - set(attributes["db.query.complexity"], "complex")
              where Len(Split(body["query_text"], "JOIN")) > 2
          - set(attributes["db.query.complexity"], "very_complex")
              where Len(Split(body["query_text"], "JOIN")) > 5
          
          # Set sampling priority based on duration
          - set(attributes["sampling.priority"], 100)
              where body["duration_ms"] > 1000
          - set(attributes["sampling.priority"], 50)
              where body["duration_ms"] > 500 and body["duration_ms"] <= 1000
          - set(attributes["sampling.priority"], 10)
              where body["duration_ms"] <= 500
              
  # Stage 2: PII sanitization
  transform/pii_sanitization:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          # Email pattern
          - replace_pattern(body["query_text"], 
              "\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b", 
              "[EMAIL_REDACTED]")
          # SSN pattern
          - replace_pattern(body["query_text"], 
              "\\b\\d{3}-\\d{2}-\\d{4}\\b", 
              "[SSN_REDACTED]")
          # Credit card pattern
          - replace_pattern(body["query_text"], 
              "\\b(?:\\d{4}[- ]?){3}\\d{4}\\b", 
              "[CC_REDACTED]")
              
  # Stage 3: Entity synthesis for New Relic
  resource/entity_synthesis:
    attributes:
      # Database entity attributes
      - key: "entity.guid"
        value: "DATABASE|${env:DEPLOYMENT_ENV}|${database_name}"
        action: upsert
        
      - key: "entity.type"
        value: "DATABASE"
        action: insert
        
      - key: "entity.name"
        from_attribute: "database_name"
        action: insert
        
      # Service correlation
      - key: "service.name"
        value: "database-intelligence-postgresql"
        action: insert
        
      - key: "service.namespace"
        value: "${env:DEPLOYMENT_ENV}"
        action: insert
        
      # Infrastructure correlation
      - key: "host.id"
        value: "${env:HOSTNAME}"
        action: insert
        
      # New Relic specific attributes
      - key: "instrumentation.provider"
        value: "opentelemetry"
        action: insert
        
      - key: "telemetry.sdk.name"
        value: "opentelemetry"
        action: insert
        
      - key: "telemetry.sdk.language"
        value: "collector"
        action: insert
        
      - key: "telemetry.sdk.version"
        value: "0.89.0"
        action: insert
        
  # Stage 4: Circuit breaker implementation
  transform/circuit_breaker:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          # Track consecutive errors per database
          - set(cache["cb_errors_" + body["database_name"]], 
              Int(cache["cb_errors_" + body["database_name"]]) + 1)
              where body["error"] == true
          
          # Reset on success
          - set(cache["cb_errors_" + body["database_name"]], 0)
              where body["error"] != true
              
          # Open circuit after threshold
          - set(cache["cb_state_" + body["database_name"]], "open")
              where Int(cache["cb_errors_" + body["database_name"]]) > 5
              
          # Drop data when circuit is open
          - set(attributes["drop"], true)
              where cache["cb_state_" + body["database_name"]] == "open"
              
  # Stage 5: Adaptive sampling
  probabilistic_sampler/adaptive:
    hash_seed: 22
    attribute_source: record
    from_attribute: "sampling.priority"
    
  # Stage 6: Attribute management
  attributes/cleanup:
    actions:
      # Remove high-cardinality attributes
      - key: backend_pid
        action: delete
      - key: transaction_id
        action: delete
      - key: snapshot_xmin
        action: delete
      
      # Hash sensitive attributes
      - key: client_addr
        action: hash
      - key: username
        action: hash
        
  # Stage 7: Metric generation for monitoring
  metricgeneration:
    rules:
      - name: db.query.duration
        type: histogram
        unit: ms
        value_attribute: duration_ms
        attributes:
          - database_name
          - db.query.complexity
          - wait_event_type
          
      - name: db.transaction.duration
        type: histogram
        unit: ms
        value_attribute: transaction_duration_ms
        attributes:
          - database_name
          - application_name
          
  # Stage 8: Cardinality control via grouping
  groupbyattrs:
    keys:
      - database_name
      - db.query.fingerprint
      - db.query.complexity
      - wait_event_type
      - connection_state
      
  # Stage 9: Optimized batching for New Relic
  batch/newrelic_optimized:
    timeout: 30s
    send_batch_size: 5000      # 50% of NR limit
    send_batch_max_size: 9000  # 90% of NR limit
    
    # Different settings per telemetry type
    metrics:
      timeout: 60s  # Less frequent for metrics
    traces:
      timeout: 5s   # More frequent for traces
    logs:
      timeout: 10s  # Balanced for logs

exporters:
  # New Relic optimized exporter
  otlp/newrelic:
    endpoint: "${env:OTLP_ENDPOINT:-https://otlp.nr-data.net:4318}"
    protocol: "http/protobuf"  # Better than gRPC for database workloads
    compression: zstd           # Better compression for JSON plans
    
    headers:
      api-key: "${env:NEW_RELIC_LICENSE_KEY}"
      "X-Source": "database-intelligence-mvp"
      "X-Version": "2.0.0"
      
    timeout: 25s  # Must be less than batch timeout
    
    # Advanced queue configuration
    sending_queue:
      enabled: true
      num_consumers: 10      # Parallel processing
      queue_size: 5000       # Handle bursts
      storage: file_storage  # Persist during restarts
      
    # Retry configuration
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      randomization_factor: 0.5
      multiplier: 1.5
      max_interval: 30s
      max_elapsed_time: 300s
      
  # Debug exporter for troubleshooting
  debug:
    verbosity: basic
    sampling_initial: 10
    sampling_thereafter: 100
    
  # Prometheus for self-monitoring
  prometheusremotewrite:
    endpoint: "${env:PROMETHEUS_ENDPOINT:-http://prometheus:9090/api/v1/write}"
    resource_to_telemetry_conversion:
      enabled: true

service:
  # Extensions to enable
  extensions: [health_check, pprof, zpages, file_storage]
  
  # Telemetry configuration
  telemetry:
    logs:
      level: "${env:LOG_LEVEL:-info}"
      encoding: json
      output_paths: ["stdout", "/var/log/otel-collector.log"]
      
    metrics:
      level: detailed
      address: 0.0.0.0:8888
      
  # Processing pipelines
  pipelines:
    # Main database intelligence pipeline
    logs/database:
      receivers: [sqlquery/postgresql_plans]
      processors:
        - memory_limiter
        - transform/query_normalization
        - transform/pii_sanitization
        - resource/entity_synthesis
        - transform/circuit_breaker
        - probabilistic_sampler/adaptive
        - attributes/cleanup
        - groupbyattrs
        - batch/newrelic_optimized
      exporters: [otlp/newrelic]
      
    # Metrics pipeline for monitoring
    metrics/database:
      receivers: [sqlquery/postgresql_plans]
      processors:
        - memory_limiter
        - metricgeneration
        - batch/newrelic_optimized
      exporters: [otlp/newrelic]
      
    # Self-monitoring pipeline
    metrics/internal:
      receivers: [prometheus]
      processors:
        - memory_limiter
        - resource/entity_synthesis
        - batch/newrelic_optimized
      exporters: [prometheusremotewrite]